{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "865d4807-cb1d-41b4-896f-600e8b02b7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\zytal\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (1.1.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\zytal\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from scikit-learn) (1.22.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\zytal\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\zytal\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from scikit-learn) (1.8.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\zytal\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from scikit-learn) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2af9c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>last_trending_date</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>publish_hour</th>\n",
       "      <th>category_id</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>...</th>\n",
       "      <th>tag_appeared_in_title</th>\n",
       "      <th>title</th>\n",
       "      <th>tags</th>\n",
       "      <th>description</th>\n",
       "      <th>trend_day_count</th>\n",
       "      <th>trend.publish.diff</th>\n",
       "      <th>trend_tag_highest</th>\n",
       "      <th>trend_tag_total</th>\n",
       "      <th>tags_count</th>\n",
       "      <th>subscriber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2kyS6SvSYSE</td>\n",
       "      <td>2017-11-20</td>\n",
       "      <td>2017-11-13</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>CaseyNeistat</td>\n",
       "      <td>2564903</td>\n",
       "      <td>96321</td>\n",
       "      <td>7972</td>\n",
       "      <td>22149</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>WE WANT TO TALK ABOUT OUR MARRIAGE</td>\n",
       "      <td>SHANtell martin</td>\n",
       "      <td>SHANTELL'S CHANNEL - https://www.youtube.com/s...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9086142.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ZAPwfrtAFY</td>\n",
       "      <td>2017-11-20</td>\n",
       "      <td>2017-11-13</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>LastWeekTonight</td>\n",
       "      <td>6109402</td>\n",
       "      <td>151250</td>\n",
       "      <td>11508</td>\n",
       "      <td>19820</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>The Trump Presidency: Last Week Tonight with J...</td>\n",
       "      <td>last week tonight trump presidency|last week t...</td>\n",
       "      <td>One year after the presidential election, John...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>65</td>\n",
       "      <td>69</td>\n",
       "      <td>4</td>\n",
       "      <td>5937292.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5qpjK5DgCt4</td>\n",
       "      <td>2017-11-20</td>\n",
       "      <td>2017-11-12</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "      <td>Rudy Mancuso</td>\n",
       "      <td>5315471</td>\n",
       "      <td>187303</td>\n",
       "      <td>7278</td>\n",
       "      <td>9990</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>Racist Superman | Rudy Mancuso, King Bach &amp; Le...</td>\n",
       "      <td>racist superman|rudy|mancuso|king|bach|racist|...</td>\n",
       "      <td>WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http...</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>68</td>\n",
       "      <td>426</td>\n",
       "      <td>23</td>\n",
       "      <td>4191209.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id last_trending_date publish_date  publish_hour  category_id  \\\n",
       "0  2kyS6SvSYSE         2017-11-20   2017-11-13            17           22   \n",
       "1  1ZAPwfrtAFY         2017-11-20   2017-11-13             7           24   \n",
       "2  5qpjK5DgCt4         2017-11-20   2017-11-12            19           23   \n",
       "\n",
       "     channel_title    views   likes  dislikes  comment_count  ...  \\\n",
       "0     CaseyNeistat  2564903   96321      7972          22149  ...   \n",
       "1  LastWeekTonight  6109402  151250     11508          19820  ...   \n",
       "2     Rudy Mancuso  5315471  187303      7278           9990  ...   \n",
       "\n",
       "   tag_appeared_in_title                                              title  \\\n",
       "0                  False                 WE WANT TO TALK ABOUT OUR MARRIAGE   \n",
       "1                  False  The Trump Presidency: Last Week Tonight with J...   \n",
       "2                   True  Racist Superman | Rudy Mancuso, King Bach & Le...   \n",
       "\n",
       "                                                tags  \\\n",
       "0                                    SHANtell martin   \n",
       "1  last week tonight trump presidency|last week t...   \n",
       "2  racist superman|rudy|mancuso|king|bach|racist|...   \n",
       "\n",
       "                                         description trend_day_count  \\\n",
       "0  SHANTELL'S CHANNEL - https://www.youtube.com/s...               7   \n",
       "1  One year after the presidential election, John...               7   \n",
       "2  WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http...               7   \n",
       "\n",
       "  trend.publish.diff trend_tag_highest  trend_tag_total  tags_count  \\\n",
       "0                  7                 2                2           1   \n",
       "1                  7                65               69           4   \n",
       "2                  8                68              426          23   \n",
       "\n",
       "   subscriber  \n",
       "0   9086142.0  \n",
       "1   5937292.0  \n",
       "2   4191209.0  \n",
       "\n",
       "[3 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "df = pd.read_csv(\"USvideos_modified.csv\", sep=\",\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b41e8d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4525, 23)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filling in descriptions that were blank\n",
    "df.description= df.description.fillna('No description provided')\n",
    "df.tags= df.tags.fillna('')\n",
    "df = df.dropna() #no information provided for subscribers to we eliminate those rows\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0da6e2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zytal\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count=CountVectorizer()\n",
    "\n",
    "count.fit(df['title'])\n",
    "\n",
    "count.get_feature_names()\n",
    "\n",
    "count_tranform=count.transform(df['title'])\n",
    "# Need to convert into Array\n",
    "cv_array=count_tranform.toarray()\n",
    "cv_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8d5554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "limt_cv_title=CountVectorizer(stop_words='english',max_features=50,min_df=2)\n",
    "limt_cv_title.fit(df['title'])\n",
    "cv_transformed_title = limt_cv_title.fit_transform(df['title'])\n",
    "cv_array_title = cv_transformed_title.toarray()\n",
    "cv_df_title=pd.DataFrame(cv_array_title,columns=limt_cv_title.get_feature_names()).add_prefix(\"\")\n",
    "#limt_cv_title.get_feature_names()\n",
    "#for col_name in cv_df_title.columns: \n",
    "    #print(col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74feb027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zytal\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "limt_cv_tags=CountVectorizer(stop_words='english',max_features=50,min_df=2)\n",
    "limt_cv_tags.fit(df['tags'])\n",
    "cv_transformed_tags = limt_cv_tags.fit_transform(df['tags'])\n",
    "cv_array_tags = cv_transformed_tags.toarray()\n",
    "cv_df_tags=pd.DataFrame(cv_array_tags,columns=limt_cv_tags.get_feature_names()).add_prefix(\"\")\n",
    "#limt_cv_tags.get_feature_names()\n",
    "#for col_name in cv_df_tags.columns: \n",
    "    #print(col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a77596a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zytal\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "limt_cv_des=CountVectorizer(stop_words='english',max_features=50,min_df=1)\n",
    "limt_cv_des.fit(df['description'])\n",
    "cv_transformed_des = limt_cv_des.fit_transform(df['description'])\n",
    "cv_array_des = cv_transformed_des.toarray()\n",
    "cv_df_des=pd.DataFrame(cv_array_des,columns=limt_cv_des.get_feature_names()).add_prefix(\"\")\n",
    "#limt_cv_des.get_feature_names()\n",
    "#for col_name in cv_df_des.columns: \n",
    "    #print(col_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28d9987-fad3-4782-8635-0fe16249bc6c",
   "metadata": {},
   "source": [
    "# Count Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4f02be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>2017</td>\n",
       "      <td>amzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>2018</td>\n",
       "      <td>bit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>beauty</td>\n",
       "      <td>cbs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>audio</td>\n",
       "      <td>best</td>\n",
       "      <td>channel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>best</td>\n",
       "      <td>black</td>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>big</td>\n",
       "      <td>bowl</td>\n",
       "      <td>facebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>black</td>\n",
       "      <td>cat</td>\n",
       "      <td>gl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bowl</td>\n",
       "      <td>celebrity</td>\n",
       "      <td>goo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>challenge</td>\n",
       "      <td>christmas</td>\n",
       "      <td>google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>christmas</td>\n",
       "      <td>comedy</td>\n",
       "      <td>http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>commercial</td>\n",
       "      <td>diy</td>\n",
       "      <td>https</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>day</td>\n",
       "      <td>dog</td>\n",
       "      <td>instagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>espn</td>\n",
       "      <td>ellen</td>\n",
       "      <td>jimmy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ft</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>late</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>game</td>\n",
       "      <td>food</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hd</td>\n",
       "      <td>funny</td>\n",
       "      <td>list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>highlights</td>\n",
       "      <td>game</td>\n",
       "      <td>live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>house</td>\n",
       "      <td>interview</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>james</td>\n",
       "      <td>iphone</td>\n",
       "      <td>ly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>jedi</td>\n",
       "      <td>james</td>\n",
       "      <td>make</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>john</td>\n",
       "      <td>late</td>\n",
       "      <td>music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>life</td>\n",
       "      <td>life</td>\n",
       "      <td>nbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>like</td>\n",
       "      <td>live</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>live</td>\n",
       "      <td>make</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>look</td>\n",
       "      <td>makeup</td>\n",
       "      <td>nfacebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>love</td>\n",
       "      <td>movie</td>\n",
       "      <td>nfl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>make</td>\n",
       "      <td>movies</td>\n",
       "      <td>nfollow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>makeup</td>\n",
       "      <td>music</td>\n",
       "      <td>nget</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>man</td>\n",
       "      <td>nbc</td>\n",
       "      <td>nhttp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>movie</td>\n",
       "      <td>new</td>\n",
       "      <td>nhttps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>music</td>\n",
       "      <td>news</td>\n",
       "      <td>ni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>netflix</td>\n",
       "      <td>night</td>\n",
       "      <td>night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>new</td>\n",
       "      <td>official</td>\n",
       "      <td>ninstagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>news</td>\n",
       "      <td>pop</td>\n",
       "      <td>nlike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>nfl</td>\n",
       "      <td>review</td>\n",
       "      <td>nsubscribe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>official</td>\n",
       "      <td>science</td>\n",
       "      <td>nthe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>refinery29</td>\n",
       "      <td>season</td>\n",
       "      <td>ntwitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>review</td>\n",
       "      <td>sp</td>\n",
       "      <td>nwatch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>season</td>\n",
       "      <td>star</td>\n",
       "      <td>smarturl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>star</td>\n",
       "      <td>super</td>\n",
       "      <td>subscribe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>super</td>\n",
       "      <td>talk</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>time</td>\n",
       "      <td>trailer</td>\n",
       "      <td>tumblr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>trailer</td>\n",
       "      <td>trump</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>trump</td>\n",
       "      <td>tutorial</td>\n",
       "      <td>video</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>video</td>\n",
       "      <td>tv</td>\n",
       "      <td>videos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>vs</td>\n",
       "      <td>video</td>\n",
       "      <td>watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>wars</td>\n",
       "      <td>videos</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>week</td>\n",
       "      <td>wars</td>\n",
       "      <td>www</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>world</td>\n",
       "      <td>world</td>\n",
       "      <td>youtu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>year</td>\n",
       "      <td>youtube</td>\n",
       "      <td>youtube</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Title           Tags Description\n",
       "0           10           2017        amzn\n",
       "1         2017           2018         bit\n",
       "2         2018         beauty         cbs\n",
       "3        audio           best     channel\n",
       "4         best          black         com\n",
       "5          big           bowl    facebook\n",
       "6        black            cat          gl\n",
       "7         bowl      celebrity         goo\n",
       "8    challenge      christmas      google\n",
       "9    christmas         comedy        http\n",
       "10  commercial            diy       https\n",
       "11         day            dog   instagram\n",
       "12        espn          ellen       jimmy\n",
       "13          ft  entertainment        late\n",
       "14        game           food        like\n",
       "15          hd          funny        list\n",
       "16  highlights           game        live\n",
       "17       house      interview        love\n",
       "18       james         iphone          ly\n",
       "19        jedi          james        make\n",
       "20        john           late       music\n",
       "21        life           life         nbc\n",
       "22        like           live         new\n",
       "23        live           make        news\n",
       "24        look         makeup   nfacebook\n",
       "25        love          movie         nfl\n",
       "26        make         movies     nfollow\n",
       "27      makeup          music        nget\n",
       "28         man            nbc       nhttp\n",
       "29       movie            new      nhttps\n",
       "30       music           news          ni\n",
       "31     netflix          night       night\n",
       "32         new       official  ninstagram\n",
       "33        news            pop       nlike\n",
       "34         nfl         review  nsubscribe\n",
       "35    official        science        nthe\n",
       "36  refinery29         season    ntwitter\n",
       "37      review             sp      nwatch\n",
       "38      season           star    smarturl\n",
       "39        star          super   subscribe\n",
       "40       super           talk        time\n",
       "41        time        trailer      tumblr\n",
       "42     trailer          trump     twitter\n",
       "43       trump       tutorial       video\n",
       "44       video             tv      videos\n",
       "45          vs          video       watch\n",
       "46        wars         videos       world\n",
       "47        week           wars         www\n",
       "48       world          world       youtu\n",
       "49        year        youtube     youtube"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_comparison = pd.DataFrame(cv_df_title.columns, columns=['Title'])\n",
    "count_comparison[\"Tags\"]= cv_df_tags.columns\n",
    "count_comparison[\"Description\"]= cv_df_des.columns\n",
    "count_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e2b1dbf-6e06-4643-ab0a-5095b220bc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_search_high(x):\n",
    "    dfs = df.reset_index()  # make sure indexes pair with number of rows\n",
    "    highest=0\n",
    "    for index, row in dfs.iterrows():\n",
    "        if x in row[\"title\"].lower().split():\n",
    "            if row[\"trend_tag_highest\"]>highest:\n",
    "                highest = row[\"trend_day_count\"]\n",
    "    return highest\n",
    "\n",
    "appendage=[]\n",
    "for x in count_comparison[\"Title\"]:\n",
    "    appendage.append(title_search_high(x))\n",
    "titlewords=pd.DataFrame(count_comparison[\"Title\"])\n",
    "titlewords[\"Highest_Trending_Day_Count\"]=appendage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a44e57ca-4848-43e0-882c-18c609a7f8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_search_high(x):\n",
    "    dfs = df.reset_index()  # make sure indexes pair with number of rows\n",
    "    highest=0\n",
    "    for index, row in dfs.iterrows():\n",
    "        if x in row[\"tags\"].lower().split():\n",
    "            if row[\"trend_tag_highest\"]>highest:\n",
    "                highest = row[\"trend_day_count\"]\n",
    "    return highest\n",
    "\n",
    "appendage=[]\n",
    "for x in count_comparison[\"Tags\"]:\n",
    "    appendage.append(tags_search_high(x))\n",
    "tagswords=pd.DataFrame(count_comparison[\"Tags\"])\n",
    "tagswords[\"Highest_Trending_Day_Count\"]=appendage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "793d47b6-989c-457c-93c5-b49d2d4d4a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def des_search_high(x):\n",
    "    dfs = df.reset_index()  # make sure indexes pair with number of rows\n",
    "    highest=0\n",
    "    for index, row in dfs.iterrows():\n",
    "        if x in row[\"tags\"].lower().split():\n",
    "            if row[\"trend_tag_highest\"]>highest:\n",
    "                highest = row[\"trend_day_count\"]\n",
    "    return highest\n",
    "\n",
    "appendage=[]\n",
    "for x in count_comparison[\"Description\"]:\n",
    "    appendage.append(des_search_high(x))\n",
    "deswords=pd.DataFrame(count_comparison[\"Description\"])\n",
    "deswords[\"Highest_Trending_Day_Count\"]=appendage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b876f7d-8341-4807-a78c-654a23d5e93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#titlewords\n",
    "#tagswords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2121d1a-5200-47f9-9a5a-b388b635c47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = titlewords.drop('Highest_Trending_Day_Count', axis=1)\n",
    "y = titlewords['Highest_Trending_Day_Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8eadb93-93e9-446f-bf6f-fd13ed18fa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_features = [\"Title\"]\n",
    "str_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"str\", str_transformer, str_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b152f5be-e654-485a-968f-d43465bb0c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title\n",
      "model score: 0.400\n",
      "model score: 0.400\n"
     ]
    }
   ],
   "source": [
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())]\n",
    ")\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())]\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "print(\"Title\")\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5df1de9c-1a4a-4788-b64c-9fe4c2af78eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags\n",
      "model score: 0.200\n",
      "model score: 0.200\n"
     ]
    }
   ],
   "source": [
    "X = tagswords.drop('Highest_Trending_Day_Count', axis=1)\n",
    "y = tagswords['Highest_Trending_Day_Count']\n",
    "\n",
    "str_features = [\"Tags\"]\n",
    "str_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"str\", str_transformer, str_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())]\n",
    ")\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())]\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "print(\"Tags\")\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b07d757-4eaf-414e-99c9-d7af16971261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description\n",
      "model score: 0.600\n",
      "model score: 0.600\n"
     ]
    }
   ],
   "source": [
    "X = deswords.drop('Highest_Trending_Day_Count', axis=1)\n",
    "y = deswords['Highest_Trending_Day_Count']\n",
    "\n",
    "str_features = [\"Description\"]\n",
    "str_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"str\", str_transformer, str_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())]\n",
    ")\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())]\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "print(\"Description\")\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c5b455-f79d-4da3-a3ea-f5ee54a7b028",
   "metadata": {},
   "source": [
    "#### NOTE\n",
    "This where the model stops working "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4c929b2-ca31-4dac-9336-d5f47029abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_feature = [\"Description\"]\n",
    "X_train, X_test = X_train[subset_feature], X_test[subset_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3e97ea8-ff1c-40ba-a8e8-be7b40315ed9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 feature(s) (shape=(40, 0)) while a minimum of 1 is required by LogisticRegression.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m ColumnTransformer(\n\u001b[0;32m      4\u001b[0m     transformers\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      5\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m\"\u001b[39m, str_transformer, selector(dtype_include\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[0;32m      6\u001b[0m     ]\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m clf \u001b[38;5;241m=\u001b[39m Pipeline(\n\u001b[0;32m      9\u001b[0m     steps\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m, preprocessor), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m, LogisticRegression())]\n\u001b[0;32m     10\u001b[0m )\n\u001b[1;32m---> 13\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel score: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m clf\u001b[38;5;241m.\u001b[39mscore(X_test, y_test))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\pipeline.py:382\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    381\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 382\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1138\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m-> 1138\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1140\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1146\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\base.py:596\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    594\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 596\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    597\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\utils\\validation.py:1074\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1069\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1070\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1071\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1072\u001b[0m     )\n\u001b[1;32m-> 1074\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1088\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1090\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1092\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\utils\\validation.py:918\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    916\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m<\u001b[39m ensure_min_features:\n\u001b[1;32m--> 918\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    919\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m feature(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    920\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m a minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    921\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_features, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_features, context)\n\u001b[0;32m    922\u001b[0m         )\n\u001b[0;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmay_share_memory(array, array_orig):\n\u001b[0;32m    925\u001b[0m     array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(array, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder)\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 feature(s) (shape=(40, 0)) while a minimum of 1 is required by LogisticRegression."
     ]
    }
   ],
   "source": [
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", str_transformer, selector(dtype_include=\"category\")),\n",
    "    ]\n",
    ")\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())]\n",
    ")\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a98d7ea-867d-4e8d-abe9-87defcfcf556",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector(dtype_include=\"category\")(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d0f8af-e114-4fe2-a097-dcff05206a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"preprocessor__num__imputer__strategy\": [\"mean\", \"median\"],\n",
    "    \"classifier__C\": [0.1, 1.0, 10, 100],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=10)\n",
    "grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927eba23-909b-430b-86db-71d266823db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\")\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba1bd89-3d3f-4cec-aec9-9820fc12dd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Internal CV score: {grid_search.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6473522-8661-4718-b3f3-c99e2472480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_results = cv_results.sort_values(\"mean_test_score\", ascending=False)\n",
    "cv_results[\n",
    "    [\n",
    "        \"mean_test_score\",\n",
    "        \"std_test_score\",\n",
    "        \"param_preprocessor__num__imputer__strategy\",\n",
    "        \"param_classifier__C\",\n",
    "    ]\n",
    "].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db344b33-46f9-4206-a680-9c1f07e91de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    (\n",
    "        \"best logistic regression from grid search: %.3f\"\n",
    "        % grid_search.score(X_test, y_test)\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
